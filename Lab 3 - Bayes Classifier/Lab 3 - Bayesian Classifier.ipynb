{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Classifier\n",
    "In this lab, you will implement and assess the performance of the Bayesian Classifier.\n",
    "\n",
    "## Lab Instructions:\n",
    "1. Read the explanation above each requirement very well\n",
    "2. Read the requirement very well before jumping into the code.\n",
    "3. Some requirements have essay questions in them, make sure you do NOT miss them.\n",
    "4. PLEASE Read the hints! They are clear and made to help you complete the requirement as fast as you should "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### always keep all your imports in the first cell ####\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import csv\n",
    "import math\n",
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement\n",
    "\n",
    "In this requirement, you will build the Bayesian Classifier and test its performance. \n",
    "\n",
    "You are provided with a data file **data1.csv** containing list of points and their corresponding classes. The format of the data files is shown in the table below.\n",
    "\n",
    "| |Class|Feature 1|Feature 1| \n",
    "|-|-|-|-|\n",
    "|Point#1|1|0.271633|-2.93224|\n",
    "|Point#2|1|7.020786|-1.98966|\n",
    "|Point#3|1|2.901827|-0.91291|\n",
    "\n",
    "\n",
    "You are also provided with a test data file **test_data.csv**. The file contains test points that are unlabelled (i.e. the class to which they belong is unknown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV(fileName):\n",
    "    csvData = pd.read_csv(fileName, sep = \",\", header = None)\n",
    "    return np.asarray(csvData)\n",
    "\n",
    "# TODO [1] : Read the file 'data1.csv' into the variable data.\n",
    "# data contains the training data together with labelled classes.\n",
    "def read_data(file_name):\n",
    "    ## HINT 1: How is the data ordered in the file?\n",
    "    ## HINT 2: Do you need to cast the data you read from the file?\n",
    "    data = readCSV('data1.csv')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test_data():\n",
    "    \n",
    "    # TODO [2.A]: Read the file 'test_data.csv' into the variable test_data\n",
    "    # test_data contains the unlabelled test class.\n",
    "    ## HINT: Do you need to cast the data you read from the file?\n",
    "\n",
    "    test_data = readCSV('test_data.csv')\n",
    "    \n",
    "    # TODO [2.B]: Read the file 'test_data_true.csv' into the variable test_data_true\n",
    "    # test_data_true contains the actual classes of the test instances, which you will compare\n",
    "    # against your predicted classes.\n",
    "    ## HINT: Do you need to cast the data you read from the file?\n",
    "\n",
    "    test_data_true = readCSV('test_data_true.csv')\n",
    "    \n",
    "    return test_data, test_data_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Terminlology\n",
    "Machine learning problems use common termonology (names and notiations) to refer to certain things. It is useful to use this termonology throughout your code to make it readable.\n",
    "\n",
    "| | |\n",
    "|:-|:--- |\n",
    "|$M$:|A scalar; represents the number of training points in the training set.|\n",
    "|$K$:|A scalar; represents the number of test points in the test set.|\n",
    "|$N$:|A scalar; represents the number of features of training set/test set (dimensionality of data).|\n",
    "|$X$:|A numpy array of shape $(M \\times N)$ containing the training data **without** its labels, where $M$ is the number of training points and $N$ is the number of features in the dataset (or dimensionality of features). <br/> Each element in $X$ is a tuple $(X_1, X_2, \\dots, X_N)$ where $N$ is the number of features in the dataset.| \n",
    "|$X_{test}$:| A numpy array of shape $(K \\times N)$ containing the test data, where $K$ is the number of test points and $N$ is the number of features in the dataset (or dimensionality of features). <br/> Each element in $X_{test}$ is a tuple $(X_1, X_2, \\dots, X_N)$ where $N$ is the number of features in the dataset. <br/> The number of columns in $X_{test}$ is equal to the number of columns in $X$ (because they have the same number of features). However, the number of rows in $X_{test}$ is different to the number of rows in $X$.|\n",
    "|**$Y$:| A numpy array of shape $(M \\times 1)$ containing the labels of the training data. Each row in $Y$ corresponds to the label of the training point in $X$.<br/> For example, $Y[j]$ corresponds to the label of the training point $X[j]$ where $0<=j<M$.|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO [3]: Fill the values of M, K, N, X, XTest, and Y respectively.\n",
    "# Do not fill them manually (i.e. do not set N = 3). They should be generic for any input file.  \n",
    "training_data = read_data('data1.csv')\n",
    "test_data, test_data_true = read_test_data()\n",
    "\n",
    "numClasses = 3\n",
    "M = training_data.shape[0] # Number of Training Points\n",
    "N = test_data.shape[1] # Number of Features\n",
    "K = test_data.shape[0] # Number of Test Points\n",
    "\n",
    "X = training_data[:, 1:] # Training Data without its labels\n",
    "X_Test = test_data # (K x N) Test Data\n",
    "Y = training_data[:, 0] # Labels of the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO [4]: Draw a scatter plot for traning data, where each class is coloured by the colour corresponding \n",
    "#           to its index in the colors array.\n",
    "# Class 1 should be coloured in red, Class 2 should be coloured in green, and Class 3 should be coloured in blue.\n",
    "# Hint: We have done a similar plot in the previous lab. What operation do we need to select training data \n",
    "#       belonging to a certain class?\n",
    "\n",
    "colors = ['r', 'g', 'b', 'c', 'y']\n",
    "\n",
    "# Training data for each class without labels\n",
    "C1 = X[Y == 1]\n",
    "C2 = X[Y == 2]\n",
    "C3 = X[Y == 3]\n",
    "\n",
    "# Scatter Plot\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Training Data')\n",
    "\n",
    "plt.scatter(C1[:, 0], C1[:, 1], c = colors[0])\n",
    "plt.scatter(C2[:, 0], C2[:, 1], c = colors[1])\n",
    "plt.scatter(C3[:, 0], C3[:, 1], c = colors[2])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    Your Answer:\\n    \\n    1 - The dataset consists of three main clusters each corresponding to data belong to one class.\\n\\n    2 - The overlap between the green cluster and the other two clusters is minimal compared to the \\n        overlap between the blue and red clusters.\\n\\n    3 - A linear decision boundary is sufficient to discern between any two classes even if the classification\\n        error won't be zero (especially between blue and red).\\n\\n    4 - The red cluster exhibits the highest variance among the three, followed by the blue. The feature causing\\n        most of the variance in them is Feature 1.\\n\\n    5 - The covariance between the two features for data in the green cluster is minimal and we may expect a diagonal\\n        covariance matrix there that's proportional to the identity matrix. For blue it should be close to diagonal and\\n        for red it's not expected to be diagonal.\\n\\n    6 - The blue cluster doesn't seem to be as dense as the other two clusters.\\n\\n    7 - Although the green cluster is reasonably dense, there is a potential outlier that's taking a value of about 20\\n        for the second feature\\n    \\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## What do you notice about the plot? (Write everything you can think of)\n",
    "'''\n",
    "    Your Answer:\n",
    "    \n",
    "    1 - The dataset consists of three main clusters each corresponding to data belong to one class.\n",
    "\n",
    "    2 - The overlap between the green cluster and the other two clusters is minimal compared to the \n",
    "        overlap between the blue and red clusters.\n",
    "\n",
    "    3 - A linear decision boundary is sufficient to discern between any two classes even if the classification\n",
    "        error won't be zero (especially between blue and red).\n",
    "\n",
    "    4 - The red cluster exhibits the highest variance among the three, followed by the blue. The feature causing\n",
    "        most of the variance in them is Feature 1.\n",
    "\n",
    "    5 - The covariance between the two features for data in the green cluster is minimal and we may expect a diagonal\n",
    "        covariance matrix there that's proportional to the identity matrix. For blue it should be close to diagonal and\n",
    "        for red it's not expected to be diagonal.\n",
    "\n",
    "    6 - The blue cluster doesn't seem to be as dense as the other two clusters.\n",
    "\n",
    "    7 - Although the green cluster is reasonably dense, there is a potential outlier that's taking a value of about 20\n",
    "        for the second feature\n",
    "    \n",
    "''' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Classifier\n",
    "The Bayesian Classifier calculates the probability of the test point belonging to each class, then the class with highest probability is assigned to the test point.\n",
    "\n",
    "Classification of $x_{test}$ = $argmax_{i} P\\big(C_i|x_{test}\\big)$ = $argmax_{i} P(x|C_i) * P(C_i)$\n",
    "\n",
    "* $P(C_i|x_{test})$: Posterior probability\n",
    "* $P(x|C_i)$: Class-conditional probability (or distribution)\n",
    "* $P(C_i)$: Class apriori probability\n",
    "                \n",
    "**Note that** $P(C_i|x_{test}) \\neq P(x_{test}|C_i) * P(C_i)$. Instead,  $P(C_i|x_{test}) = \\frac{P(x_{test}|C_i) * P(C_i)}{P(x_{test})}$. However, when we compare multiple classes, the denominator $P(x_{test})$ is independent of the class $i$ and can be regarded as normalizing factor.\n",
    "\n",
    "**We start by** computing statistical parameters about each class from the data. \n",
    "\n",
    "For each class, we are interested in **three** parameters that will be used for calculating the Gaussian class-conditional distribution and the posterior probability.\n",
    "\n",
    "These parameters are:\n",
    "\n",
    "|||\n",
    "|:-|:-|\n",
    "|**Class Apriori Probability: ($P_C$)**| A scalar; the probability of class occurence (how frequent this class appears in the training data)|\n",
    "|**Class Mean: ($\\mu$)**| A vector of shape $(N \\times 1)$, it is the expected value (mean) calculated from the training points of each class.|\n",
    "|**Class Covariance Matrix: ($\\Sigma$)**| A square symmetric matrix of shape $(N \\times N)$ representing the covariances between all the feature calculated from the training points of the class. <br/> For example: Matrix element $\\sigma^2_{12}$ is the covariance between the 1st and the 2nd features|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pClasses = [] # A list of size (numClasses, 1) containing the a priori probabilities of each class in the training set.\n",
    "\n",
    "estimate_means = [] # A numpy array of size (numClasses, N) containing the mean points of each class in the training set. \n",
    "                    # HINT: USE NP.MEAN\n",
    "\n",
    "estimate_covariances = [] # A numpy array of size (numClasses, N, N) containing the covariance matrices of each class in the training set.\n",
    "                          # HINT: USE NP.COV (Pay attenention for what it takes as an argument)\n",
    "                          \n",
    "X_class = [C1, C2, C3]\n",
    "                             \n",
    "for classIndex in range(numClasses):\n",
    "    # TODO [5]: Estimate the parameters of the Gaussian distributions of the given classes.\n",
    "    # Fill pClasses, estimate_means, and estimate_covariances in this part \n",
    "    # Your code should be vectorized WITHOUT USING A SINGLE FOR LOOP.\n",
    "    classNumber = classIndex + 1\n",
    "    numOfLabels = len(Y)\n",
    "    probability = np.sum(Y == classNumber) / numOfLabels\n",
    "    pClasses.append(probability)\n",
    "    \n",
    "    mean_1 = np.mean(X_class[classIndex][:, 0])\n",
    "    mean_2 = np.mean(X_class[classIndex][:, 1])\n",
    "    estimate_means.append([mean_1, mean_2])\n",
    "    \n",
    "    covariance = np.cov(X_class[classIndex].T)\n",
    "    estimate_covariances.append(covariance)\n",
    "    \n",
    "\n",
    "estimate_means = np.array(estimate_means)\n",
    "estimate_covariances = np.array(estimate_covariances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Sum of apriori probabilities should be 1, found 0.9999999999999999",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Github\\Neural-Networks-Labs\\Lab 3 - Bayes Classifier\\Lab 3 - Bayesian Classifier.ipynb Cell 12\u001b[0m in \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Github/Neural-Networks-Labs/Lab%203%20-%20Bayes%20Classifier/Lab%203%20-%20Bayesian%20Classifier.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m### Test your implementation ###\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Github/Neural-Networks-Labs/Lab%203%20-%20Bayes%20Classifier/Lab%203%20-%20Bayesian%20Classifier.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m### DO NOT CHANGE THIS CODE ###\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Github/Neural-Networks-Labs/Lab%203%20-%20Bayes%20Classifier/Lab%203%20-%20Bayesian%20Classifier.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(pClasses) \u001b[39m==\u001b[39m numClasses,\\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Github/Neural-Networks-Labs/Lab%203%20-%20Bayes%20Classifier/Lab%203%20-%20Bayesian%20Classifier.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIncorrect class apriori probability list, it should be of length \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(pClasses))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Github/Neural-Networks-Labs/Lab%203%20-%20Bayes%20Classifier/Lab%203%20-%20Bayesian%20Classifier.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39msum(pClasses)\u001b[39m==\u001b[39m\u001b[39m1\u001b[39m,\\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Github/Neural-Networks-Labs/Lab%203%20-%20Bayes%20Classifier/Lab%203%20-%20Bayesian%20Classifier.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mSum of apriori probabilities should be 1, found \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(np\u001b[39m.\u001b[39msum(pClasses))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Github/Neural-Networks-Labs/Lab%203%20-%20Bayes%20Classifier/Lab%203%20-%20Bayesian%20Classifier.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39massert\u001b[39;00m estimate_means\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (numClasses, N),\\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Github/Neural-Networks-Labs/Lab%203%20-%20Bayes%20Classifier/Lab%203%20-%20Bayesian%20Classifier.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIncorrect estimated means, it should be of shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat((numClasses, N))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Github/Neural-Networks-Labs/Lab%203%20-%20Bayes%20Classifier/Lab%203%20-%20Bayesian%20Classifier.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39massert\u001b[39;00m estimate_covariances\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (numClasses, N, N),\\\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Github/Neural-Networks-Labs/Lab%203%20-%20Bayes%20Classifier/Lab%203%20-%20Bayesian%20Classifier.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIncorrect estimate covariance matrices, it should be of shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat((numClasses, N, N))\n",
      "\u001b[1;31mAssertionError\u001b[0m: Sum of apriori probabilities should be 1, found 0.9999999999999999"
     ]
    }
   ],
   "source": [
    "### Test your implementation ###\n",
    "### DO NOT CHANGE THIS CODE ###\n",
    "assert len(pClasses) == numClasses,\\\n",
    "        'Incorrect class apriori probability list, it should be of length {}'.format(len(pClasses))\n",
    "assert np.sum(pClasses)==1,\\\n",
    "        'Sum of apriori probabilities should be 1, found {}'.format(np.sum(pClasses))\n",
    "\n",
    "assert estimate_means.shape == (numClasses, N),\\\n",
    "        'Incorrect estimated means, it should be of shape {}'.format((numClasses, N))\n",
    "assert estimate_covariances.shape == (numClasses, N, N),\\\n",
    "        'Incorrect estimate covariance matrices, it should be of shape {}'.format((numClasses, N, N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The second step** in the classifier is to calculate the class-conditional density using the Gaussian destribution:\n",
    "\n",
    "$P(x|C_i) = \\mathcal{N}(x; \\mu_i, \\Sigma_i) = \\frac{1}{(2\\pi)^{\\frac{N}{2}}|\\Sigma_i|^{\\frac{1}{2}}} exp\\big(\\frac{-1}{2}(x-\\mu_i)^T\\Sigma^{-1}_{i}(x-\\mu_i)\\big)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 6: Implement the multivariate normal gaussian distribution with parameters mu and sigma, and return the\n",
    "#  value in prob.\n",
    "def multivariate_normal_gaussian(X, mu, sigma):\n",
    "    det = np.linalg.det(sigma)\n",
    "    inv = np.linalg.inv(sigma)\n",
    "    L = len(mu)\n",
    "    d = (2 * np.pi)**(L/2) * det**(0.5)\n",
    "    prob = np.exp(-0.5 * ((X - mu).T @ inv @ (X - mu))) / d\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Test your implementation ###\n",
    "### DO NOT CHANGE THIS CODE ###\n",
    "np.random.seed(90)\n",
    "assertion_x = np.random.rand(3).reshape(-1,1)\n",
    "assertion_mu = np.random.rand(3).reshape(-1,1)\n",
    "assertion_sigma = np.random.rand(9).reshape(3,3)\n",
    "assertion_probability = multivariate_normal_gaussian(assertion_x, assertion_mu, assertion_sigma)[0][0]\n",
    "assertion_probability = round(assertion_probability, 1)\n",
    "\n",
    "assert assertion_probability == 7.8,\\\n",
    "    'Incorrect Gaussian Probability calculated'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The final step** is to go for each test point, calculate its posterior probability against each class, then classify it to the class with the highest posterior probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO [7]: Apply the Bayesian Classifier to predict the classes of the test points.\n",
    "predicted_classes = [] # predicted_classes: A numpy array of size (K, 1) where K is the number of points in the test set. Every element in this array\n",
    "                       # contains the predicted class of Bayes classifier for this test point.\n",
    "\n",
    "# P(Ci|x): Posterior Probability\n",
    "# P(x|Ci): Class-conditional Probability\n",
    "# P(Ci): Class Probability \n",
    "\n",
    "for i in range(X_Test.shape[0]):\n",
    "    # print(\"For test point:\", X_Test[i])\n",
    "    classProbabilities = np.zeros(numClasses)\n",
    "    # TODO [7.A]: Compute the probability that the test point X_Test[i] belongs to each class in numClasses.\n",
    "    #  Fill the array classProbabilities accordingly.\n",
    "    for classIndex in range(numClasses):\n",
    "        conditionalProb = multivariate_normal_gaussian(X_Test[i], estimate_means[classIndex], estimate_covariances[classIndex])\n",
    "        classProb = pClasses[classIndex]\n",
    "        posteriorProb = conditionalProb * classProb # P(x|Ci) * P(Ci)\n",
    "        classProbabilities[classIndex] = posteriorProb\n",
    "        \n",
    "    \n",
    "    # TODO [7.B]: Find the prediction of the test point X_Test[i] and append it to the predicted_classes array.\n",
    "    prediction = np.argmax(classProbabilities) + 1\n",
    "    predicted_classes.append(prediction)\n",
    "\n",
    "    # print('Class Probabilities: ', classProbabilities)  # the first class is the left most in the scatter plot\n",
    "    # print(\"Predicted class is :\", predicted_classes[i])\n",
    "    # print(\"******************************************************************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 95.56%\n"
     ]
    }
   ],
   "source": [
    "# TODO [8]: Compute the accuracy of the generated Bayesian classifier \n",
    "# WITHOUT USING ANY FOR LOOPs.\n",
    "\n",
    "# We need to compare the predicted classes to test_data_true\n",
    "true_arr = np.array(test_data_true.reshape(-1)); \n",
    "predicted_arr = np.array(predicted_classes);\n",
    "result = (true_arr == predicted_arr)\n",
    "accuracy = sum(result) / len(result)\n",
    "print('Accuracy = ' + str(round(accuracy, 4) * 100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO [9]: Generate a 3D-plot for the generated distributions. x-axis and y-axis represent the features of the data, \n",
    "#           where z-axis represent the Gaussian probability N at this point.\n",
    "\n",
    "x = np.linspace(-10, 10, 300)\n",
    "y = np.linspace(-10, 15, 300)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.zeros(X.shape)\n",
    "\n",
    "for i in range(Z.shape[0]):\n",
    "    for j in range(Z.shape[1]):\n",
    "        # TODO [9]: Fill in the matrix Z which will represent the probability distribution of every point.\n",
    "        # Z[i,j] represents the joint probability N(x,y) for x = i and y = j. \n",
    "        # We want to draw the gaussian probability N(x,y) for all points. \n",
    "        Z[i, j] = 0\n",
    "        for classIndex in range(numClasses):\n",
    "            prob = multivariate_normal_gaussian([X[i, j], Y[i, j]], estimate_means[classIndex], estimate_covariances[classIndex])\n",
    "            Z[i, j] = max(Z[i, j], prob)\n",
    "\n",
    "# Make a 3D plot, do not change code\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.plot_surface(X, Y, Z, cmap='viridis', linewidth=0)\n",
    "ax.set_xlabel('X axis')\n",
    "ax.set_ylabel('Y axis')\n",
    "ax.set_zlabel('Z axis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How can you judge your plot is correct?\n",
    "'''\n",
    "    Your Answer:\n",
    "    By comparing to the dataset's scatterplot (at the specific class we're plotting for). \n",
    "    The Gaussian should be centered at the mean of the points and the way it's skewed/scaled depends on the covariance.\n",
    "    If the data was spread in a certain direction, the covariance parameter will make the Gaussian skew/scale so that\n",
    "    it spreads in a similar way.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
